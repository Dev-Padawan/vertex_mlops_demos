{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ecd8dd-4575-436e-a16b-b7b052446c55",
   "metadata": {},
   "source": [
    "## 02 - Custom model training on Vertex Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2c1a6-6488-485a-b790-989f9538d4c9",
   "metadata": {},
   "source": [
    "This demo uses the Vertex AI, our end-to-end managed ML platform on Google Cloud. Vertex AI integrates Google's ML offerings across Google Cloud into a seamless development experience. In addition to model training and deployment services, Vertex AI also includes a variety of MLOps products, including Vertex Pipelines (the focus of this lab), Model Monitoring, Feature Store, and more. You can see all Vertex AI product offerings in the diagram below.\n",
    "\n",
    "<img src=\"images/vertex-overview_1920.png\"/>\n",
    "\n",
    "#### Why are ML pipelines useful?\n",
    "\n",
    "Before we dive in, let's first understand why you would want to use a pipeline. Imagine you're building out a ML workflow that includes processing data, training a model, hyperparameter tuning, evaluation, and model deployment. Each of these steps may have different dependencies, which may become unwieldy if you treat the entire workflow as a monolith. As you begin to scale your ML process, you might want to share your ML workflow with others on your team so they can run it and contribute code. Without a reliable, reproducible process, this can become difficult. With pipelines, each step in your ML process is its own container. This lets you develop steps independently and track the input and output from each step in a reproducible way. You can also schedule or trigger runs of your pipeline based on other events in your Cloud environment, like kicking off a pipeline run when new training data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b17eda-41fb-4d86-90a8-a76cee89846b",
   "metadata": {},
   "source": [
    "#### Vertex Pipelines setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be373a9-c50c-4f3d-9340-464e0cfe9bd5",
   "metadata": {},
   "source": [
    "There are a few additional libraries we'll need to install in order to use Vertex Pipelines:\n",
    "\n",
    "- __Kubeflow Pipelines__: This is the SDK we'll be using to build our pipeline. Vertex Pipelines supports running pipelines built with both Kubeflow Pipelines or TFX.\n",
    "- __Google Cloud Pipeline Components__: This library provides pre-built components that make it easier to interact with Vertex AI services from your pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab06619-033c-4bd8-bc6c-47fa01f39968",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174dccb-b9e1-4d07-a534-1d2136943b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install {USER_FLAG} google-cloud-aiplatform --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f3826-fdfc-4930-a33a-dcf99853327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install {USER_FLAG} google-cloud-pipeline-components --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6f5bd-d2fb-47fc-b810-e20b3662cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install {USER_FLAG} kfp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829a8ce-2d2e-4908-a32f-6cab01f32f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip3 install {USER_FLAG} google-cloud-aiplatform --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892a6ab-8a2d-47d5-aefc-f5c3f8cd7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pip install --upgrade pip\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb99cc3-429e-41aa-838e-cf5fe5d799af",
   "metadata": {},
   "source": [
    "__Note:__ you may need to restart the kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876256e5-edf9-4e6c-acc5-2c916dd6ee12",
   "metadata": {},
   "source": [
    "#### Set your project ID and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf38c4e-6172-42ea-b1c2-1307e1c63f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ibnd-argls-cstmr-demos\"\n",
    "BUCKET_NAME=\"gs://ibnd-argls-ml-demos-storage/02_mlops_scikit_demo\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "1061843a-1354-4978-a4e5-8c287a320117",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9199448-bb41-48ae-85c4-afc45182dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "# from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "raw",
   "id": "818e4568-7807-41f1-8c7b-14b92e451f72",
   "metadata": {},
   "source": [
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824eccc-2931-442b-b790-11b22b0052d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9e332-2533-4423-82c0-2b7b375d41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/{TIMESTAMP}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b91dd98-0243-40db-a8c5-678e7a9252cd",
   "metadata": {},
   "source": [
    "#### Create a Python function based component\n",
    "Using the KFP SDK, we can create components based on Python functions. We'll use that for the 3 components in our first pipeline. We'll first build the product_name component, which simply takes a string as input and returns that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d43aa-c932-4375-925d-b0707385f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", output_component_file=\"first-component.yaml\")\n",
    "def product_name(text: str) -> str:\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de16a2-6753-43f3-ad60-0ee949d175f9",
   "metadata": {},
   "source": [
    "Let's take a closer look at the syntax here:\n",
    "\n",
    "- The `@component` decorator compiles this function to a component when the pipeline is run. You'll use this anytime you write a custom component.\n",
    "- The `base_image` parameter specifies the container image this component will use.\n",
    "- The `output_component_file` parameter is optional, and specifies the yaml file to write the compiled component to. After running the cell you should see that file written to your notebook instance. If you wanted to share this component with someone, you could send them the generated yaml file and have them load it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35c9bd-c5f7-45aa-bb8e-15d63c2eca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name_component = kfp.components.load_component_from_file('./first-component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b451b-cbc7-4552-836a-4d4e52ca7255",
   "metadata": {},
   "source": [
    "- The `-> str` after the function definition specifies the output type for this component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397c7b95-5b96-479c-a9cf-6a5c7a7a2a31",
   "metadata": {},
   "source": [
    "#### Create two additional components\n",
    "To complete our pipeline, we'll create two more components. The first one we'll define takes a string as input, and converts this string to its corresponding emoji if there is one. It returns a tuple with the input text passed, and the resulting emoji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da224d9c-d434-45e0-aa7f-e9c120a3d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"emoji\"])\n",
    "def emoji(\n",
    "    text: str,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"emoji_text\", str),  # Return parameters\n",
    "        (\"emoji\", str),\n",
    "    ],\n",
    "):\n",
    "    import emoji\n",
    "\n",
    "    emoji_text = text\n",
    "    emoji_str = emoji.emojize(':' + emoji_text + ':', language='alias')\n",
    "    print(\"output one: {}; output_two: {}\".format(emoji_text, emoji_str))\n",
    "    return (emoji_text, emoji_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c645c6-4bb8-44fe-aac2-12b7031fa7a1",
   "metadata": {},
   "source": [
    "This component is a bit more complex than our previous one. Let's break down what's new:\n",
    "\n",
    "- The `packages_to_install` paramater tells the component any external library dependencies for this container. In this case, we're using a library called emoji.\n",
    "- This component returns a `NamedTuple` called `Outputs`. Notice that each of the strings in this tuple have keys: `emoji_text` and `emoji`. We'll use these in our next component to access the output.\n",
    "\n",
    "The final component in this pipeline will consume the output of the first two and combine them to return a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d7fd6-d4b2-4289-b0a7-1fab9d54007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def build_sentence(\n",
    "    product: str,\n",
    "    emoji: str,\n",
    "    emojitext: str\n",
    ") -> str:\n",
    "    print(\"We completed the pipeline, hooray!\")\n",
    "    end_str = product + \" is \"\n",
    "    if len(emoji) > 0:\n",
    "        end_str += emoji\n",
    "    else:\n",
    "        end_str += emojitext\n",
    "    return(end_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267fc52-7ac9-47c2-8572-502ff31a7759",
   "metadata": {},
   "source": [
    "#### Putting the components together into a pipeline\n",
    "The component definitions we defined above created factory functions that can be used in a pipeline definition to create steps. To set up a pipeline, use the `@pipeline` decorator, give the pipeline a name and description, and provide the root path where your pipeline's artifacts should be written. By artifacts, we mean any output files generated by your pipeline. This intro pipeline doesn't generate any, but our next pipeline will.\n",
    "\n",
    "In the next block of code we define an `intro_pipeline` function. This is where we specify the inputs to our initial pipeline steps, and how steps connect to each other:\n",
    "\n",
    "- `product_task` takes a product name as input. Here we're passing \"Vertex Pipelines\" but you can change this to whatever you'd like.\n",
    "- `emoji_task` takes the text code for an emoji as input. You can also change this to whatever you'd like. For example, \"party_face\" refers to the ðŸ¥³ emoji. Note that since both this and the product_task component don't have any steps that feed input into them, we manually specify the input for these when we define our pipeline.\n",
    "- The last step in our pipeline - `consumer_task` has three input parameters:\n",
    "> - The output of `product_task`. Since this step only produces one output, we can reference it via `product_task.output`.\n",
    "> - The `emoji` output of our `emoji_task` step. See the `emoji` component defined above where we named the output parameters.\n",
    "> - Similarly, the `emoji_text` named output from the `emoji` component. In case our pipeline is passed text that doesn't correspond with an emoji, it'll use this text to construct a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e466b9-fc8c-4beb-a746-d224376b9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name=\"hello-world\",\n",
    "    description=\"An intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "# You can change the `text` and `emoji_str` parameters here to update the pipeline output\n",
    "def intro_pipeline(text: str = \"Vertex Pipelines\", emoji_str: str = \"sparkles\"):\n",
    "    product_task = product_name(text)\n",
    "    emoji_task = emoji(emoji_str)\n",
    "    consumer_task = build_sentence(\n",
    "        product_task.output,\n",
    "        emoji_task.outputs[\"emoji\"],\n",
    "        emoji_task.outputs[\"emoji_text\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125181b7-3267-4439-8bfa-dcc24ccd8da1",
   "metadata": {},
   "source": [
    "#### Compile and run the pipeline\n",
    "With your pipeline defined, you're ready to compile it. The following will generate a JSON file that you'll use to run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e0ce9-72aa-4386-ae59-eb04bf4719f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=intro_pipeline, package_path=\"intro_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fad0a4-a7df-4e61-861b-e6f594c2aa92",
   "metadata": {},
   "source": [
    "Then define your pipeline job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f12ede-519b-434e-9d0a-4b9abedd1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"hello-world-pipeline\",\n",
    "    template_path=\"intro_pipeline_job.json\",\n",
    "    job_id=\"hello-world-pipeline-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b290a-a343-4c40-986a-56987777d363",
   "metadata": {},
   "source": [
    "Finally, run the job to create a new pipeline execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5660eac-b1d7-455c-b9b1-965481662e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b1518-4188-4a2c-a63f-adfb8cab0797",
   "metadata": {},
   "source": [
    "<img src=\"images/Screenshot_2024-02-01.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1beed8-ea64-4166-af0d-245f5ea471bd",
   "metadata": {},
   "source": [
    "## 02 - End-to-End ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c61daa-2dee-4272-b4cd-11d4415cf11c",
   "metadata": {},
   "source": [
    "In this pipeline, we'll use the UCI Machine Learning Dry beans dataset, from: KOKLU, M. and OZKAN, I.A., (2020), \"Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.\"ÂIn Computers and Electronics in Agriculture, 174, 105507. DOI.\n",
    "\n",
    "This is a tabular dataset, and in our pipeline we'll use the dataset to train, evaluate, and deploy an AutoML model that classifies beans into one of 7 types based on their characteristics.\n",
    "\n",
    "This pipeline will:\n",
    "\n",
    "- Create a Dataset in Vertex AI\n",
    "- Train a tabular classification model with AutoML\n",
    "- Get evaluation metrics on this model\n",
    "- Based on the evaluation metrics, decide whether to deploy the model using conditional logic in Vertex Pipelines\n",
    "- Deploy the model to an endpoint using Vertex Prediction\n",
    "\n",
    "Each of the steps outlined will be a component. Most of the pipeline steps will use pre-built components for Vertex AI services via the `google_cloud_pipeline_components` library we imported earlier in this codelab. In this section, we'll define one custom component first. Then, we'll define the rest of the pipeline steps using pre-built components. Pre-built components make it easier to access Vertex AI services, like model training and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565035aa-12e9-48a4-8388-06f8cc264290",
   "metadata": {},
   "source": [
    "#### A custom component for model evaluation\n",
    "The custom component we'll define will be used towards the end of our pipeline once model training has completed. This component will do a few things:\n",
    "\n",
    "- Get the evaluation metrics from the trained AutoML classification model\n",
    "- Parse the metrics and render them in the Vertex Pipelines UI\n",
    "- Compare the metrics to a threshold to determine whether the model should be deployed\n",
    "\n",
    "Before we define the component, let's understand its input and output parameters. As input, this pipeline takes some metadata on our Cloud project, the resulting trained model (we'll define this component later), the model's evaluation metrics, and a `thresholds_dict_str`. The `thresholds_dict_str` is something we'll define when we run our pipeline. In the case of this classification model, this will be the area under the ROC curve value for which we should deploy the model. For example, if we pass in 0.95, that means we'd only like our pipeline to deploy the model if this metric is above 95%.\n",
    "\n",
    "Our evaluation component returns a string indicating whether or not to deploy the model. Add the following in a notebook cell to create this custom component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4618b9-2f02-4b15-90ef-2281ef85c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    output_component_file=\"tabular_eval_component.yaml\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def classification_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,  # \"us-central1\",\n",
    "    api_endpoint: str,  # \"us-central1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str,\n",
    "    model: Input[Artifact],\n",
    "    metrics: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform as aip\n",
    "\n",
    "    # Fetch model eval info\n",
    "    def get_eval_info(client, model_name):\n",
    "        from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "        response = client.list_model_evaluations(parent=model_name)\n",
    "        metrics_list = []\n",
    "        metrics_string_list = []\n",
    "        for evaluation in response:\n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation.name)\n",
    "            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n",
    "            metrics = MessageToDict(evaluation._pb.metrics)\n",
    "            for metric in metrics.keys():\n",
    "                logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "            metrics_str = json.dumps(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "            metrics_string_list.append(metrics_str)\n",
    "\n",
    "        return (\n",
    "            evaluation.name,\n",
    "            metrics_list,\n",
    "            metrics_string_list,\n",
    "        )\n",
    "\n",
    "    # Use the given metrics threshold(s) to determine whether the model is\n",
    "    # accurate enough to deploy.\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\n",
    "        for k, v in thresholds_dict.items():\n",
    "            logging.info(\"k {}, v {}\".format(k, v))\n",
    "            if k in [\"auRoc\", \"auPrc\"]:  # higher is better\n",
    "                if metrics_dict[k] < v:  # if under threshold, don't deploy\n",
    "                    logging.info(\"{} < {}; returning False\".format(metrics_dict[k], v))\n",
    "                    return False\n",
    "        logging.info(\"threshold checks passed.\")\n",
    "        return True\n",
    "\n",
    "    def log_metrics(metrics_list, metricsc):\n",
    "        test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\n",
    "        logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\n",
    "\n",
    "        # log the ROC curve\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "        thresholds = []\n",
    "        for item in metrics_list[0][\"confidenceMetrics\"]:\n",
    "            fpr.append(item.get(\"falsePositiveRate\", 0.0))\n",
    "            tpr.append(item.get(\"recall\", 0.0))\n",
    "            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\n",
    "        print(f\"fpr: {fpr}\")\n",
    "        print(f\"tpr: {tpr}\")\n",
    "        print(f\"thresholds: {thresholds}\")\n",
    "        metricsc.log_roc_curve(fpr, tpr, thresholds)\n",
    "\n",
    "        # log the confusion matrix\n",
    "        annotations = []\n",
    "        for item in test_confusion_matrix[\"annotationSpecs\"]:\n",
    "            annotations.append(item[\"displayName\"])\n",
    "        logging.info(\"confusion matrix annotations: %s\", annotations)\n",
    "        metricsc.log_confusion_matrix(\n",
    "            annotations,\n",
    "            test_confusion_matrix[\"rows\"],\n",
    "        )\n",
    "\n",
    "        # log textual metrics info as well\n",
    "        for metric in metrics_list[0].keys():\n",
    "            if metric != \"confidenceMetrics\":\n",
    "                val_string = json.dumps(metrics_list[0][metric])\n",
    "                metrics.log_metric(metric, val_string)\n",
    "        # metrics.metadata[\"model_type\"] = \"AutoML Tabular classification\"\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    aip.init(project=project)\n",
    "    # extract the model resource name from the input Model Artifact\n",
    "    model_resource_path = model.metadata[\"resourceName\"]\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    client = aip.gapic.ModelServiceClient(client_options=client_options)\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(\n",
    "        client, model_resource_path\n",
    "    )\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\n",
    "    log_metrics(metrics_list, metricsc)\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n",
    "    if deploy:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "\n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2715b69d-2fdb-439b-bfbd-75629f1fa8f0",
   "metadata": {},
   "source": [
    "#### Adding Google Cloud pre-built components\n",
    "In this step we'll define the rest of our pipeline components and see how they all fit together. First, define the display name for your pipeline run using a timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0d5c6-0fe7-4187-8f0a-62a40158f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "DISPLAY_NAME = 'automl-beans{}'.format(str(int(time.time())))\n",
    "print(DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23043f-9703-4199-88d9-eedeabf97f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"automl-tab-beans-training-v2\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_source: str = \"bq://ibnd-argls-cstmr-demos.mlops_demo.beans\",\n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str = '{\"auRoc\": 0.95}',\n",
    "):\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project, display_name=display_name, bq_source=bq_source\n",
    "    )\n",
    "\n",
    "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=1000,\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"Area\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Perimeter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MajorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MinorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"AspectRation\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Eccentricity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ConvexArea\"}},\n",
    "            {\"numeric\": {\"column_name\": \"EquivDiameter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Extent\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Solidity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"roundness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Compactness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor1\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor2\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor3\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor4\"}},\n",
    "            {\"categorical\": {\"column_name\": \"Class\"}},\n",
    "        ],\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Class\",\n",
    "    )\n",
    "    model_eval_task = classification_model_eval_metrics(\n",
    "        project,\n",
    "        gcp_region,\n",
    "        api_endpoint,\n",
    "        thresholds_dict_str,\n",
    "        training_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    with dsl.Condition(\n",
    "        model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "\n",
    "        endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            display_name=\"train-automl-beans\",\n",
    "        )\n",
    "\n",
    "        gcc_aip.ModelDeployOp(\n",
    "            model=training_op.outputs[\"model\"],\n",
    "            endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4ce41-8e4f-4e62-af60-b01c36a82a22",
   "metadata": {},
   "source": [
    "Let's see what's happening in this code:\n",
    "\n",
    "- First, just as in our previous pipeline, we define the input parameters this pipeline takes. We need to set these manually since they don't depend on the output of other steps in the pipeline.\n",
    "- The rest of the pipeline uses a few pre-built components for interacting with Vertex AI services:\n",
    "> - `TabularDatasetCreateOp` creates a tabular dataset in Vertex AI given a dataset source either in Cloud Storage or BigQuery. In this pipeline, we're passing the data via a BigQuery table URL\n",
    "> - `AutoMLTabularTrainingJobRunOp` kicks off an AutoML training job for a tabular dataset. We pass a few configuration parameters to this component, including the model type (in this case, classification), some data on the columns, how long we'd like to run training for, and a pointer to the dataset. Notice that to pass in the dataset to this component, we're providing the output of the _previous component_ via `dataset_create_op.outputs[\"dataset\"]`\n",
    "> - `EndpointCreateOp` creates an endpoint in Vertex AI. The endpoint created from this step will be passed as input to the next component\n",
    "> - `ModelDeployOp` deploys a given model to an endpoint in Vertex AI. In this case, we're using the endpoint created from the previous step. There are additional configuration options available, but here we're providing the endpoint machine type and model we'd like to deploy. We're passing in the model by accessing the outputs of the training step in our pipeline\n",
    "- This pipeline also makes use of conditional logic, a feature of Vertex Pipelines that lets you define a condition, along with different branches based on the result of that condition. Remember that when we defined our pipeline we passed a `thresholds_dict_str` parameter. This is the accuracy threshold we're using to determine whether to deploy our model to an endpoint. To implement this, we make use of the `Condition` class from the KFP SDK. The condition we pass in is the output of the custom eval component we defined earlier. If this condition is true, the pipeline will continue to execute the `deploy_op` component. If accuracy doesn't meet our predefined threshold, the pipeline will stop here and won't deploy a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ae2b9-2001-4b38-83fe-9b51c629a29d",
   "metadata": {},
   "source": [
    "#### Compile and run the end-to-end ML pipeline\n",
    "With our full pipeline defined, it's time to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cb100-be2d-4d95-935e-e1825dd76ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"tab_classif_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5126675-d1b0-47c0-99be-01deab6dc5f6",
   "metadata": {},
   "source": [
    "Next, define the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7e4ac-2bf5-442f-885f-9155230eb60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"automl-tab-beans-training\",\n",
    "    template_path=\"tab_classif_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"project\": PROJECT_ID, \"display_name\": DISPLAY_NAME},\n",
    "    enable_caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4bf9d-ef2a-4dd7-b905-5e7989a5334a",
   "metadata": {},
   "source": [
    "And finally, run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d8a48-eeeb-4eeb-9ce7-9642991ab3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca9705-a3b5-4890-b15e-55d0cb6c0bc1",
   "metadata": {},
   "source": [
    "<img src=\"images/Screenshot_2024-02-01_12.14.02â€¯_AM.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24b00e-a099-4bd8-8b80-fb54dd23daef",
   "metadata": {},
   "source": [
    "#### Comparing metrics across pipeline runs\n",
    "If you run this pipeline multiple times, you may want to compare metrics across runs. You can use the aiplatform.get_pipeline_df() method to access run metadata. Here, we'll get metadata for all runs of this pipeline and load it into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3be205-d0bb-4e93-ae90-6fb56e325f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_df = aiplatform.get_pipeline_df(pipeline=\"automl-tab-beans-training-v2\")\n",
    "small_pipeline_df = pipeline_df.head(2)\n",
    "small_pipeline_df"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
