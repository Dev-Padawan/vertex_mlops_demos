{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Vertex AI SDK for Python: AutoML training video object tracking model for batch prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl"
   },
   "source": [
    "This demo demonstrates how to use the Vertex AI SDK for Python to create video object tracking models and do batch prediction using a Google Cloud [AutoML](https://cloud.google.com/vertex-ai/docs/start/automl-users) model.\n",
    "\n",
    "Learn more about [Object tracking for video data](https://cloud.google.com/vertex-ai/docs/training-overview#object_tracking_for_videos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:automl,training,batch_prediction"
   },
   "source": [
    "### Objective\n",
    "\n",
    "This demo shows how to create an AutoML video object tracking model from a Python script, and then do a batch prediction using the Vertex AI SDK. Alternatively, you can create and deploy models using the `gcloud` command-line tool or online using the Cloud Console.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Vertex `Dataset` resource.\n",
    "- Train the model.\n",
    "- View the model evaluation.\n",
    "- Make a batch prediction.\n",
    "\n",
    "There is one key difference between using batch prediction and using online prediction:\n",
    "\n",
    "* Prediction Service: Does an on-demand prediction for the entire set of instances (i.e., one or more data items) and returns the results in real-time.\n",
    "\n",
    "* Batch Prediction Service: Does a queued (batch) prediction for the entire set of instances in the background and stores the results in a Cloud Storage bucket when ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:traffic,vot"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [Traffic](https://storage.googleapis.com/automl-video-demo-data/traffic_videos/traffic_videos_labels.csv) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"ibnd-argls-cstmr-demos\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "# BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"} ibnd-argls-cstmr-demos-vertexai-vision/06-sdk-objct-tracking\n",
    "BUCKET_URI = \"gs://ibnd-argls-cstmr-demos-vertexai-vision/06-sdk-objct-tracking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:automl"
   },
   "source": [
    "#### Start creating the AutoML video object tracking model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,csv"
   },
   "source": [
    "#### Location of Cloud Storage training data.\n",
    "\n",
    "Now set the variable `IMPORT_FILE` to the location of the CSV index file in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:traffic,csv,vot"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"gs://cloud-samples-data/ai-platform-unified/video/traffic/traffic_videos_labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_peek:csv"
   },
   "source": [
    "#### Quick peek at your data\n",
    "\n",
    "Start by doing a quick peek at the data. You count the number of examples by counting the number of rows in the CSV index file  (`wc -l`) and then peek at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick_peek:csv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples 685\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,sedan,1565750291672021,11.933333,0.509205,0.594283,,,0.728737,0.760959,,\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,pickup_suv_van,1565750291672171,17.566666,0.761241,0.498466,,,0.948839,0.668524,,\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,pickup_suv_van,1565750291672223,20.433333,0.000000,0.465235,,,0.142638,0.665644,,\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,pickup_suv_van,1565750291672347,25.766666,0.486523,0.592331,,,0.720611,0.776687,,\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,pickup_suv_van,1565750291672575,28.966666,0.578534,0.652778,,,0.828647,0.862967,,\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,pickup_suv_van,1565750291672549,28.966666,0.000000,0.518571,,,0.148841,0.737677,,\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,pickup_suv_van,1565750291672599,28.966666,0.106979,0.458078,,,0.377877,0.678937,,\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,pickup_suv_van,1565715798494273,32.466666,0.333083,0.485473,,,0.542722,0.647774,,\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,sedan,1565715798494439,36.433333,0.935638,0.564839,,,1.000000,0.672182,,\n",
      "gs://automl-video-demo-data/traffic_videos/highway_005.mp4,pickup_suv_van,1565715798494381,36.433333,0.000000,0.455703,,,0.164878,0.660083,,\n"
     ]
    }
   ],
   "source": [
    "FILE = IMPORT_FILE\n",
    "\n",
    "count = ! gsutil cat $FILE | wc -l\n",
    "print(\"Number of Examples\", int(count[0]))\n",
    "\n",
    "! gsutil cat $FILE | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:video,vot"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `VideoDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "\n",
    "This operation may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:video,vot"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VideoDataset\n",
      "Create VideoDataset backing LRO: projects/998979163436/locations/us-central1/datasets/3006211025386078208/operations/8341554846565924864\n",
      "VideoDataset created. Resource name: projects/998979163436/locations/us-central1/datasets/3006211025386078208\n",
      "To use this VideoDataset in another session:\n",
      "ds = aiplatform.VideoDataset('projects/998979163436/locations/us-central1/datasets/3006211025386078208')\n",
      "Importing VideoDataset data: projects/998979163436/locations/us-central1/datasets/3006211025386078208\n",
      "Import VideoDataset data backing LRO: projects/998979163436/locations/us-central1/datasets/3006211025386078208/operations/5241389453074759680\n",
      "VideoDataset data imported. Resource name: projects/998979163436/locations/us-central1/datasets/3006211025386078208\n",
      "projects/998979163436/locations/us-central1/datasets/3006211025386078208\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.VideoDataset.create(\n",
    "    display_name=\"Traffic\",\n",
    "    gcs_source=[IMPORT_FILE],\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.video.object_tracking,\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:video,vot"
   },
   "source": [
    "### Create and run training pipeline\n",
    "\n",
    "To train an AutoML model, you perform two steps: 1) create a training pipeline, and 2) run the pipeline.\n",
    "\n",
    "#### Create training pipeline\n",
    "\n",
    "An AutoML training pipeline is created with the `AutoMLVideoTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
    "- `prediction_type`: The type task to train the model for.\n",
    "  - `classification`: A video classification model.\n",
    "  - `object_tracking`: A video object tracking model.\n",
    "  - `action_recognition`: A video action recognition model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_automl_pipeline:video,vot"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.training_jobs.AutoMLVideoTrainingJob object at 0x7f1e4d268a10>\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.AutoMLVideoTrainingJob(\n",
    "    display_name=\"traffic\",\n",
    "    prediction_type=\"object_tracking\",\n",
    ")\n",
    "\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_automl_pipeline:video"
   },
   "source": [
    "#### Run the training pipeline\n",
    "\n",
    "Next, you start the training job by invoking the method `run`, with the following parameters:\n",
    "\n",
    "- `dataset`: The `Dataset` resource to train the model.\n",
    "- `model_display_name`: The human readable name for the trained model.\n",
    "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
    "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
    "\n",
    "The `run` method when completed returns the `Model` resource.\n",
    "\n",
    "The execution of the training pipeline will take upto 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_automl_pipeline:video"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1255675745305362432?project=998979163436\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLVideoTrainingJob run completed. Resource name: projects/998979163436/locations/us-central1/trainingPipelines/1255675745305362432\n",
      "Model available at projects/998979163436/locations/us-central1/models/5767666364382707712\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=\"traffic\",\n",
    "    training_fraction_split=0.8,\n",
    "    test_fraction_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "source": [
    "## Review model evaluation scores\n",
    "After your model has finished training, you can review the evaluation scores for it.\n",
    "\n",
    "First, you need to get a reference to the new model. As with datasets, you can either use the reference to the model variable you created when you deployed the model or you can list all of the models in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.model_evaluation.model_evaluation.ModelEvaluation object at 0x7f1e4d17a750> \n",
      "resource name: projects/998979163436/locations/us-central1/models/5767666364382707712@1/evaluations/875566002828476416\n"
     ]
    }
   ],
   "source": [
    "model_evaluations = model.list_model_evaluations()\n",
    "model_evaluation = list(model_evaluations)[0]\n",
    "print(model_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3377983702b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's evaluation metrics from Training:\n",
      "\n",
      "metric: boundingBoxMeanAveragePrecision, value: 0.30644187\n",
      "\n",
      "metric: boundingBoxMetrics, value: [{'iouThreshold': 0.5, 'confidenceMetrics': [{'precision': 0.3993994, 'recall': 1.0, 'f1Score': 0.57081544, 'confidenceThreshold': 0.0}, {'recall': 0.14285715, 'confidenceThreshold': 0.058405496, 'f1Score': 0.10795455, 'precision': 0.08675799}, {'confidenceThreshold': 0.059411008, 'recall': 0.14285715, 'precision': 0.08715596, 'f1Score': 0.10826211}, {'recall': 0.14285715, 'precision': 0.08755761, 'f1Score': 0.108571425, 'confidenceThreshold': 0.061475746}, {'recall': 0.14285715, 'precision': 0.08796296, 'f1Score': 0.108882524, 'confidenceThreshold': 0.062099054}, {'confidenceThreshold': 0.063024625, 'recall': 0.14285715, 'f1Score': 0.109195404, 'precision': 0.0883721}, {'precision': 0.088785045, 'recall': 0.14285715, 'f1Score': 0.10951009, 'confidenceThreshold': 0.0640149}, {'f1Score': 0.10982659, 'recall': 0.14285715, 'precision': 0.089201875, 'confidenceThreshold': 0.06537615}, {'f1Score': 0.11014493, 'recall': 0.14285715, 'confidenceThreshold': 0.06618022, 'precision': 0.08962264}, {'recall': 0.14285715, 'f1Score': 0.11046512, 'precision': 0.0900474, 'confidenceThreshold': 0.06721854}, {'confidenceThreshold': 0.06738676, 'precision': 0.09047619, 'f1Score': 0.110787176, 'recall': 0.14285715}, {'f1Score': 0.11111111, 'recall': 0.14285715, 'precision': 0.09090909, 'confidenceThreshold': 0.06839761}, {'precision': 0.09134615, 'f1Score': 0.11143695, 'confidenceThreshold': 0.069799215, 'recall': 0.14285715}, {'f1Score': 0.11176471, 'precision': 0.09178744, 'confidenceThreshold': 0.06997758, 'recall': 0.14285715}, {'precision': 0.09223301, 'recall': 0.14285715, 'f1Score': 0.112094395, 'confidenceThreshold': 0.071960755}, {'f1Score': 0.112426035, 'confidenceThreshold': 0.072207786, 'precision': 0.09268293, 'recall': 0.14285715}, {'recall': 0.14285715, 'precision': 0.09313726, 'confidenceThreshold': 0.07269398, 'f1Score': 0.11275964}, {'confidenceThreshold': 0.07347826, 'recall': 0.14285715, 'precision': 0.093596056, 'f1Score': 0.11309524}, {'recall': 0.14285715, 'confidenceThreshold': 0.07421792, 'f1Score': 0.11343284, 'precision': 0.09405941}, {'confidenceThreshold': 0.07482256, 'f1Score': 0.11377245, 'precision': 0.094527364, 'recall': 0.14285715}, {'precision': 0.095, 'recall': 0.14285715, 'f1Score': 0.11411411, 'confidenceThreshold': 0.07630815}, {'f1Score': 0.11445783, 'recall': 0.14285715, 'precision': 0.09547739, 'confidenceThreshold': 0.07682988}, {'confidenceThreshold': 0.07770677, 'precision': 0.0959596, 'recall': 0.14285715, 'f1Score': 0.11480363}, {'recall': 0.14285715, 'f1Score': 0.11515152, 'confidenceThreshold': 0.07777359, 'precision': 0.0964467}, {'precision': 0.096938774, 'confidenceThreshold': 0.078196384, 'recall': 0.14285715, 'f1Score': 0.11550152}, {'recall': 0.14285715, 'precision': 0.0974359, 'f1Score': 0.11585366, 'confidenceThreshold': 0.07852889}, {'precision': 0.09793814, 'confidenceThreshold': 0.08062913, 'f1Score': 0.11620795, 'recall': 0.14285715}, {'recall': 0.14285715, 'precision': 0.098445594, 'f1Score': 0.116564415, 'confidenceThreshold': 0.08267592}, {'precision': 0.098958336, 'f1Score': 0.11692308, 'recall': 0.14285715, 'confidenceThreshold': 0.083368465}, {'precision': 0.09947644, 'recall': 0.14285715, 'confidenceThreshold': 0.08386816, 'f1Score': 0.11728395}, {'precision': 0.1, 'recall': 0.14285715, 'f1Score': 0.11764706, 'confidenceThreshold': 0.08423052}, {'f1Score': 0.11801242, 'recall': 0.14285715, 'confidenceThreshold': 0.08447302, 'precision': 0.1005291}, {'confidenceThreshold': 0.08739977, 'recall': 0.14285715, 'precision': 0.10106383, 'f1Score': 0.11838006}, {'f1Score': 0.11875, 'recall': 0.14285715, 'precision': 0.101604275, 'confidenceThreshold': 0.08777662}, {'f1Score': 0.11912226, 'confidenceThreshold': 0.08787687, 'precision': 0.10215054, 'recall': 0.14285715}, {'precision': 0.1027027, 'confidenceThreshold': 0.089413695, 'recall': 0.14285715, 'f1Score': 0.11949685}, {'precision': 0.10326087, 'confidenceThreshold': 0.08995994, 'recall': 0.14285715, 'f1Score': 0.119873814}, {'precision': 0.10382514, 'confidenceThreshold': 0.091617994, 'recall': 0.14285715, 'f1Score': 0.12025317}, {'confidenceThreshold': 0.09191993, 'recall': 0.14285715, 'precision': 0.104395606, 'f1Score': 0.12063492}, {'f1Score': 0.12101911, 'recall': 0.14285715, 'confidenceThreshold': 0.09290457, 'precision': 0.10497238}, {'f1Score': 0.12140575, 'precision': 0.10555556, 'confidenceThreshold': 0.09461102, 'recall': 0.14285715}, {'precision': 0.10614525, 'f1Score': 0.12179487, 'confidenceThreshold': 0.094857246, 'recall': 0.14285715}, {'confidenceThreshold': 0.09730895, 'precision': 0.10674157, 'recall': 0.14285715, 'f1Score': 0.1221865}, {'f1Score': 0.12258065, 'recall': 0.14285715, 'confidenceThreshold': 0.10575745, 'precision': 0.107344635}, {'confidenceThreshold': 0.10752756, 'f1Score': 0.122977346, 'recall': 0.14285715, 'precision': 0.10795455}, {'recall': 0.14285715, 'confidenceThreshold': 0.10839496, 'f1Score': 0.12337662, 'precision': 0.108571425}, {'f1Score': 0.1237785, 'recall': 0.14285715, 'confidenceThreshold': 0.10993457, 'precision': 0.109195404}, {'recall': 0.14285715, 'f1Score': 0.12418301, 'precision': 0.10982659, 'confidenceThreshold': 0.110973574}, {'f1Score': 0.124590166, 'confidenceThreshold': 0.11126988, 'recall': 0.14285715, 'precision': 0.11046512}, {'recall': 0.14285715, 'f1Score': 0.125, 'precision': 0.11111111, 'confidenceThreshold': 0.11642164}, {'confidenceThreshold': 0.11900183, 'f1Score': 0.12541254, 'recall': 0.14285715, 'precision': 0.11176471}, {'recall': 0.14285715, 'confidenceThreshold': 0.121035255, 'f1Score': 0.12582782, 'precision': 0.112426035}, {'f1Score': 0.12624584, 'precision': 0.11309524, 'confidenceThreshold': 0.1242528, 'recall': 0.14285715}, {'precision': 0.11377245, 'f1Score': 0.12666667, 'recall': 0.14285715, 'confidenceThreshold': 0.12474921}, {'confidenceThreshold': 0.12505648, 'f1Score': 0.1270903, 'precision': 0.11445783, 'recall': 0.14285715}, {'recall': 0.14285715, 'precision': 0.11515152, 'f1Score': 0.12751678, 'confidenceThreshold': 0.1268784}, {'recall': 0.14285715, 'confidenceThreshold': 0.13037571, 'f1Score': 0.12794612, 'precision': 0.11585366}, {'f1Score': 0.12837838, 'recall': 0.14285715, 'precision': 0.116564415, 'confidenceThreshold': 0.13173491}, {'confidenceThreshold': 0.13206108, 'f1Score': 0.12881356, 'precision': 0.11728395, 'recall': 0.14285715}, {'recall': 0.14285715, 'confidenceThreshold': 0.13494362, 'f1Score': 0.1292517, 'precision': 0.11801242}, {'f1Score': 0.12969284, 'precision': 0.11875, 'recall': 0.14285715, 'confidenceThreshold': 0.13639846}, {'recall': 0.14285715, 'f1Score': 0.13013698, 'confidenceThreshold': 0.14176512, 'precision': 0.11949685}, {'recall': 0.14285715, 'precision': 0.12025317, 'confidenceThreshold': 0.14213696, 'f1Score': 0.1305842}, {'precision': 0.12101911, 'confidenceThreshold': 0.14509857, 'f1Score': 0.13103448, 'recall': 0.14285715}, {'recall': 0.14285715, 'f1Score': 0.13148789, 'precision': 0.12179487, 'confidenceThreshold': 0.14576723}, {'precision': 0.12258065, 'confidenceThreshold': 0.14857733, 'recall': 0.14285715, 'f1Score': 0.13194445}, {'precision': 0.12337662, 'f1Score': 0.13240418, 'recall': 0.14285715, 'confidenceThreshold': 0.14899753}, {'confidenceThreshold': 0.14940262, 'recall': 0.14285715, 'f1Score': 0.13286713, 'precision': 0.12418301}, {'f1Score': 0.13333334, 'precision': 0.125, 'confidenceThreshold': 0.15072602, 'recall': 0.14285715}, {'precision': 0.12582782, 'f1Score': 0.13380282, 'confidenceThreshold': 0.15375951, 'recall': 0.14285715}, {'f1Score': 0.13427562, 'recall': 0.14285715, 'confidenceThreshold': 0.16672333, 'precision': 0.12666667}, {'f1Score': 0.13475177, 'precision': 0.12751678, 'recall': 0.14285715, 'confidenceThreshold': 0.17181008}, {'confidenceThreshold': 0.1733701, 'f1Score': 0.13523132, 'recall': 0.14285715, 'precision': 0.12837838}, {'recall': 0.14285715, 'confidenceThreshold': 0.21656376, 'f1Score': 0.13571429, 'precision': 0.1292517}, {'recall': 0.14285715, 'precision': 0.13013698, 'confidenceThreshold': 0.25587603, 'f1Score': 0.13620071}, {'precision': 0.13103448, 'f1Score': 0.13669065, 'confidenceThreshold': 0.26895514, 'recall': 0.14285715}, {'recall': 0.13533835, 'precision': 0.125, 'confidenceThreshold': 0.29, 'f1Score': 0.1299639}, {'f1Score': 0.20125785, 'recall': 0.120300755, 'precision': 0.61538464, 'confidenceThreshold': 0.32679725}, {'f1Score': 0.20253165, 'confidenceThreshold': 0.33853012, 'precision': 0.64, 'recall': 0.120300755}, {'recall': 0.120300755, 'confidenceThreshold': 0.35078236, 'f1Score': 0.20382166, 'precision': 0.6666667}, {'f1Score': 0.1923077, 'recall': 0.11278196, 'precision': 0.65217394, 'confidenceThreshold': 0.35477546}, {'precision': 0.6363636, 'recall': 0.10526316, 'confidenceThreshold': 0.35594267, 'f1Score': 0.18064517}, {'precision': 0.61904764, 'recall': 0.09774436, 'f1Score': 0.16883117, 'confidenceThreshold': 0.35743523}, {'recall': 0.09774436, 'f1Score': 0.16993465, 'precision': 0.65, 'confidenceThreshold': 0.35923088}, {'f1Score': 0.17105263, 'recall': 0.09774436, 'precision': 0.68421054, 'confidenceThreshold': 0.37474883}, {'precision': 0.6666667, 'f1Score': 0.1589404, 'confidenceThreshold': 0.37609825, 'recall': 0.09022556}, {'precision': 0.7058824, 'f1Score': 0.16, 'recall': 0.09022556, 'confidenceThreshold': 0.38326466}, {'precision': 0.6875, 'recall': 0.082706764, 'f1Score': 0.147651, 'confidenceThreshold': 0.38340488}, {'f1Score': 0.13513513, 'confidenceThreshold': 0.38499105, 'recall': 0.075187966, 'precision': 0.6666667}, {'precision': 0.71428573, 'confidenceThreshold': 0.38690856, 'f1Score': 0.13605443, 'recall': 0.075187966}, {'f1Score': 0.1369863, 'precision': 0.7692308, 'recall': 0.075187966, 'confidenceThreshold': 0.3940953}, {'precision': 0.8333333, 'recall': 0.075187966, 'f1Score': 0.13793103, 'confidenceThreshold': 0.39559}, {'precision': 0.8181818, 'f1Score': 0.125, 'recall': 0.067669176, 'confidenceThreshold': 0.40391994}, {'precision': 0.9, 'confidenceThreshold': 0.40588444, 'recall': 0.067669176, 'f1Score': 0.12587413}, {'recall': 0.060150377, 'f1Score': 0.112676054, 'precision': 0.8888889, 'confidenceThreshold': 0.4170242}, {'confidenceThreshold': 0.41774136, 'precision': 0.875, 'f1Score': 0.09929078, 'recall': 0.05263158}, {'recall': 0.04511278, 'precision': 0.85714287, 'f1Score': 0.08571429, 'confidenceThreshold': 0.42105484}, {'recall': 0.037593983, 'precision': 0.8333333, 'f1Score': 0.07194245, 'confidenceThreshold': 0.4667448}, {'precision': 0.8, 'recall': 0.030075189, 'confidenceThreshold': 0.4704118, 'f1Score': 0.057971016}, {'confidenceThreshold': 0.4754223, 'precision': 0.75, 'recall': 0.02255639, 'f1Score': 0.04379562}, {'f1Score': 0.029411765, 'precision': 0.6666667, 'recall': 0.015037594, 'confidenceThreshold': 0.4762897}, {'recall': 0.007518797, 'f1Score': 0.014814815, 'precision': 0.5, 'confidenceThreshold': 0.5034426}, {'f1Score': 0.014925373, 'recall': 0.007518797, 'precision': 1.0, 'confidenceThreshold': 0.510669}, {'confidenceThreshold': 1.0, 'precision': 1.0, 'f1Score': 0.0, 'recall': 0.0}], 'meanAveragePrecision': 0.30644187}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation metrics\n",
    "for evaluation in model_evaluations:\n",
    "    evaluation = evaluation.to_dict()\n",
    "    print(\"Model's evaluation metrics from Training:\\n\")\n",
    "    metrics = evaluation[\"metrics\"]\n",
    "    for metric in metrics.keys():\n",
    "        print(f\"metric: {metric}, value: {metrics[metric]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "## Send a batch prediction request\n",
    "\n",
    "Send a batch prediction to your deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_items:batch_prediction"
   },
   "source": [
    "### Get test item(s)\n",
    "\n",
    "Now do a batch prediction to your Vertex model. You will use arbitrary examples out of the dataset as a test items. Don't be concerned that the examples were likely used in training the model -- we just want to demonstrate how to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_test_items:automl,vot,csv"
   },
   "outputs": [],
   "source": [
    "test_items = ! gsutil cat $IMPORT_FILE | head -n2\n",
    "cols_1 = test_items[0].split(\",\")\n",
    "cols_2 = test_items[1].split(\",\")\n",
    "if len(cols_1) > 12:\n",
    "    test_item_1 = str(cols_1[1])\n",
    "    test_item_2 = str(cols_2[1])\n",
    "    test_label_1 = str(cols_1[2])\n",
    "    test_label_2 = str(cols_2[2])\n",
    "else:\n",
    "    test_item_1 = str(cols_1[0])\n",
    "    test_item_2 = str(cols_2[0])\n",
    "    test_label_1 = str(cols_1[1])\n",
    "    test_label_2 = str(cols_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_batch_file:automl,video"
   },
   "source": [
    "### Make a batch input file\n",
    "\n",
    "Now make a batch input file, which you store in your local Cloud Storage bucket. The batch input file can be either CSV or JSONL. You will use JSONL in this tutorial. For JSONL file, you make one dictionary entry per line for each video. The dictionary contains the key/value pairs:\n",
    "\n",
    "- `content`: The Cloud Storage path to the video.\n",
    "- `mimeType`: The content type. In our example, it is a `avi` file.\n",
    "- `timeSegmentStart`: The start timestamp in the video to do prediction on. *Note*, the timestamp must be specified as a string and followed by s (second), m (minute) or h (hour).\n",
    "- `timeSegmentEnd`: The end timestamp in the video to do prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_batch_file:automl,video"
   },
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 POST https://storage.googleapis.com/upload/storage/v1/b/ibnd-argls-cstmr-demos-vertexai-vision/06-sdk-objct-tracking/o?uploadType=multipart: Not Found: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidResponse\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_prep_and_do_upload\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[1;32m   2620\u001b[0m                 \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2621\u001b[0;31m                 \u001b[0mcommand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2622\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_do_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[1;32m   2426\u001b[0m                 \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m                 \u001b[0mcommand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_do_multipart_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[1;32m   1926\u001b[0m         response = upload.transmit(\n\u001b[0;32m-> 1927\u001b[0;31m             \u001b[0mtransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1928\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py\u001b[0m in \u001b[0;36mtransmit\u001b[0;34m(self, transport, data, metadata, content_type, timeout)\u001b[0m\n\u001b[1;32m    153\u001b[0m         return _request_helpers.wait_and_retry(\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mretriable_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_status_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/resumable_media/requests/_request_helpers.py\u001b[0m in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_CONNECTION_ERROR_CLASSES\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py\u001b[0m in \u001b[0;36mretriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/resumable_media/_upload.py\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_status_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_status_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/resumable_media/_helpers.py\u001b[0m in \u001b[0;36mrequire_status_code\u001b[0;34m(response, status_codes, get_status_code, callback)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34m\"Expected one of\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;34m*\u001b[0m\u001b[0mstatus_codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         )\n",
      "\u001b[0;31mInvalidResponse\u001b[0m: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8416/1012581440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# uploading data variable content to bucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# printing path of uploaded file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mupload_from_string\u001b[0;34m(self, data, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   3070\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3071\u001b[0m             \u001b[0mchecksum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchecksum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3072\u001b[0;31m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3073\u001b[0m         )\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mupload_from_file\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2778\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m             \u001b[0mchecksum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchecksum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2780\u001b[0;31m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2781\u001b[0m         )\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_prep_and_do_upload\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[1;32m   2623\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreated_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidResponse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2625\u001b[0;31m             \u001b[0m_raise_from_invalid_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2627\u001b[0m     def upload_from_file(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_raise_from_invalid_response\u001b[0;34m(error)\u001b[0m\n\u001b[1;32m   4789\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{response.request.method} {response.request.url}: {error_message}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4791\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 POST https://storage.googleapis.com/upload/storage/v1/b/ibnd-argls-cstmr-demos-vertexai-vision/06-sdk-objct-tracking/o?uploadType=multipart: Not Found: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>)"
     ]
    }
   ],
   "source": [
    "test_filename = \"test.jsonl\"\n",
    "gcs_input_uri = BUCKET_URI + \"/test.jsonl\"\n",
    "# making data_1 and data_2 variables using the structure mentioned above\n",
    "data_1 = {\n",
    "    \"content\": test_item_1,\n",
    "    \"mimeType\": \"video/avi\",\n",
    "    \"timeSegmentStart\": \"0.0s\",\n",
    "    \"timeSegmentEnd\": \"5.0s\",\n",
    "}\n",
    "\n",
    "data_2 = {\n",
    "    \"content\": test_item_2,\n",
    "    \"mimeType\": \"video/avi\",\n",
    "    \"timeSegmentStart\": \"0.0s\",\n",
    "    \"timeSegmentEnd\": \"5.0s\",\n",
    "}\n",
    "\n",
    "# getting reference to bucket\n",
    "bucket = storage.Client(project=PROJECT_ID).bucket(BUCKET_URI.replace(\"gs://\", \"\"))\n",
    "\n",
    "# creating a blob\n",
    "blob = bucket.blob(blob_name=test_filename)\n",
    "\n",
    "# creating data variable\n",
    "data = json.dumps(data_1) + \"\\n\" + json.dumps(data_2) + \"\\n\"\n",
    "\n",
    "# uploading data variable content to bucket\n",
    "blob.upload_from_string(data)\n",
    "\n",
    "# printing path of uploaded file\n",
    "print(gcs_input_uri)\n",
    "\n",
    "# printing content of uploaded file\n",
    "! gsutil cat $gcs_input_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request:mbsdk"
   },
   "source": [
    "### Make the batch prediction request\n",
    "\n",
    "Now that your Model resource is trained, you can make a batch prediction by invoking the batch_predict() method, with the following parameters:\n",
    "\n",
    "- `job_display_name`: The human readable name for the batch prediction job.\n",
    "- `gcs_source`: A list of one or more batch request input files.\n",
    "- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.\n",
    "- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=\"traffic\",\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=BUCKET_URI,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "print(batch_predict_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "### Wait for completion of batch prediction job\n",
    "\n",
    "Next, wait for the batch job to complete. Alternatively, one can set the parameter `sync` to `True` in the `batch_predict()` method to block until the batch prediction job is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,vot"
   },
   "source": [
    "### Get the predictions\n",
    "\n",
    "Next, get the results from the completed batch prediction job.\n",
    "\n",
    "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method iter_outputs() to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a JSON format:\n",
    "\n",
    "- `content`: The prediction request.\n",
    "- `prediction`: The prediction response.\n",
    " - `id`: The internal assigned unique identifiers for each prediction request.\n",
    " - `displayName`: The class names for the predicted label.\n",
    " - `confidences`: The predicted confidence, between 0 and 1, per class label.\n",
    " - `timeSegmentStart`: The time offset in the video to the start of the video sequence.\n",
    " - `timeSegmentEnd`: The time offset in the video to the end of the video sequence.\n",
    " - `frames`: Location with frames of the tracked object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,vot"
   },
   "outputs": [],
   "source": [
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\".replace(\n",
    "        BUCKET_URI + \"/\", \"\"\n",
    "    )\n",
    "    data = bucket.get_blob(gfile_name).download_as_string()\n",
    "    data = json.loads(data)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Model\n",
    "- AutoML Training Job\n",
    "- Batch Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "# Delete the dataset using the Vertex dataset object\n",
    "dataset.delete()\n",
    "\n",
    "# Delete the model using the Vertex model object\n",
    "model.delete()\n",
    "\n",
    "# Delete the AutoML or Pipeline training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the batch prediction job using the Vertex batch prediction object\n",
    "batch_predict_job.delete()\n",
    "\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk_automl_video_object_tracking_batch.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
