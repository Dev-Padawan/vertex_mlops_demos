{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title",
    "tags": []
   },
   "source": [
    "# Vertex AI Forecasting Model for Predicting RRS Prices in Texas \n",
    "## AutoML Pipelines (BigQueryJobOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "The Multi-Cloud Analytics Demo ingests and integrates data from multiple sources. [Vertex AI Forecasting](https://cloud.google.com/vertex-ai/docs/start/automl-users) enables users to leverage this data and easily create rich models to forecast prices or other values that vary with time and other events. It provides a no-code user-interface for Data Analysts to create state-of-the-art models, and a managed service with high-end tooling for Data Scientists (and more proficient users) to build even more sophisticated models. \n",
    "\n",
    "This notebook creates a model that forecasts the prices for Responsive Reserve Service (RRS) capacity in Texas using data from on-premises data sources, third-party data sources (including weather feeds), and historical prices from the Day Ahead Market (DAM) in Texas. \n",
    "\n",
    "This model then performs batch predictions on future prices using the model. The user may then adjust trading strategies for electric power futures contracts in Texas. \n",
    "\n",
    "### Overview of the Data \n",
    "\n",
    "The Electric Reliability Council of Texas (ERCOT) manages the flow of electric power to 23 million Texas customers – representing 85 percent of the state’s electric load. As the independent system operator for the region, ERCOT schedules power on an electric grid that connects 40,500 miles of transmission lines and more than 550 generation units. ERCOT also performs financial settlement for the competitive wholesale bulk-power market and administers retail switching for 6.6 million premises in competitive choice areas.\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "The Day-Ahead Market (DAM) is a voluntary, financially-binding forward energy market. The DAM matches willing buyers and sellers, subject to network security and other constraints, whereby energy is co-optimized with Ancillary Services and certain Congestion Revenue Rights. It provides a platform to hedge congestion costs in the day-ahead of the Operating Day, and instruments to mitigate the risk of price volatility in Real-Time.\n",
    "\n",
    "#### Ancillary Services\n",
    "\n",
    "Ancillary services are products used by ERCOT to maintain reliability minute-by-minute, 365 days per year. There are four main ancillary service products: Regulation Service – Up, Regulation Service – Down, Responsive Reserve Service, and Non-spinning Reserve Service.\n",
    "\n",
    "Regulation up and down are used to balance the grid in a near-instantaneous fashion when supply and demand fluctuate due to a variety of factors, such as weather, generation outages, wind production intermitency, and transmission outages. ERCOT uses these regulation services every hour of the year. Resources providing regulation services must comply with ERCOT instructions in less than five minutes.\n",
    "\n",
    "Responsive Reserves and Non-spinning Reserves are used by ERCOT when the grid is at, or near, a state of emergency due to inadequate generation. Resources providing Responsive Reserves must increase output in compliance with ERCOT instructions in less 10 minutes; those providing Non-spinning Reserves must comply in less than 30 minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installation (Run once per environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest version of Vertex AI SDK\n",
    "\n",
    "import os\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = \"--user\"\n",
    "else:\n",
    "    USER_FLAG = \"\"\n",
    "\n",
    "# Run once per environment\n",
    "! pip3 install --upgrade google-cloud-aiplatform[full] $USER_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once per environment\n",
    "# Install the latest GA version of google-cloud-storage library as well.\n",
    "\n",
    "! pip3 install -U google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once per environment\n",
    "# Install the latest GA version of google-cloud-pipeline-components library as well.\n",
    "\n",
    "! pip3 install $USER kfp google-cloud-pipeline-components --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart the Kernel\n",
    "\n",
    "Once, per environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Note: Multiple Runs\n",
    "\n",
    "## For repeated runs of the notebook, run from this cell downwards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the versions of the packages you installed. The KFP SDK version should be >=1.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.12\n",
      "google_cloud_pipeline_components version: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-cloud-aiplatform               1.11.0\n"
     ]
    }
   ],
   "source": [
    "# Confirm that aiplatform is installed\n",
    "\n",
    "!pip list | grep aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "\n",
    "from google.cloud import aiplatform as aip\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, Dataset, component)\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2 import compiler \n",
    "\n",
    "# Import the bigquery library\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-cloud-aiplatform               1.11.0\n"
     ]
    }
   ],
   "source": [
    "# Confirm that aiplatform is installed\n",
    "\n",
    "!pip list | grep aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220401201253\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "print(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://fsi-select-demo-ml-misc/forecast_w_pipelines_v2\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2022-03-31T16:38:09Z  gs://fsi-select-demo-ml-misc/forecast_w_pipelines_v2/#1648744689098834  metageneration=1\n",
      "TOTAL: 1 objects, 0 bytes (0 B)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"fsi-select-demo\"  # @param {type:\"string\"}\n",
    "DATA_SET_ID = \"vertex_forecasting_datasets\"\n",
    "VIEW_NAME = f\"ds-rrs_consolidated_power_demand_ercot_weather_train_v2_{TIMESTAMP}\"\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "MODEL_DISPLAY_NAME = f\"ntbk-ppln-texas-rrs-prices-forecast-model_{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk",
    "tags": []
   },
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:automl",
    "tags": []
   },
   "source": [
    "# Sample\n",
    "\n",
    "Now you are ready to start creating your own AutoML forecasting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_SQL=\"\"\"\n",
    "    SELECT\n",
    "       Delivery_Date,\n",
    "       RRS,\n",
    "       ave_temp,\n",
    "       min_temp,\n",
    "       max_temp,\n",
    "       date,\n",
    "       ds,\n",
    "       hr,\n",
    "       state,\n",
    "       demand_mgwhr\n",
    "    FROM\n",
    "      `fsi-select-demo.vertex_forecasting_datasets.consolidated_power_demand_ercot_weather_train` \n",
    "    WHERE \n",
    "      ds > '2016-06-30' and ds < '2021-06-30'\n",
    "    ORDER BY\n",
    "       date\n",
    "    DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the table exists, and if it does not, create it using the results of the query above. \n",
    "\n",
    "References: \n",
    "- __[BigQuery Usage Guide](https://googleapis.dev/python/bigquery/latest/usage/index.html#working-with-bigquery-resources)__\n",
    "- __[Python Client for Google BigQuery](https://googleapis.dev/python/bigquery/latest/index.html)__\n",
    "- __[Managing Datasets](https://googleapis.dev/python/bigquery/latest/usage/datasets.html)__\n",
    "- __[Creating and using tables](https://cloud.google.com/bigquery/docs/tables#python)__\n",
    "- __[Managing Tables](https://googleapis.dev/python/bigquery/latest/usage/tables.html)__\n",
    "- __[Google Cloud Pipeline Components list](https://cloud.google.com/vertex-ai/docs/pipelines/gcpc-list)__\n",
    "- __[TimeSeriesDatasetCreateOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.v1.dataset.html#google_cloud_pipeline_components.v1.dataset.TimeSeriesDatasetCreateOp)__\n",
    "- __[AutoMLForecastingTrainingJobRunOp](AutoMLForecastingTrainingJobRunOp)__\n",
    "- __[BigqueryQueryJobOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.v1.bigquery.html#google_cloud_pipeline_components.v1.bigquery.BigqueryQueryJobOp)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AutoML forecasting regression model pipeline that uses components from google_cloud_pipeline_components\n",
    "\n",
    "Create and deploy an AutoML tabular regression Model resource using a Dataset resource.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Service Account\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"355426521391-compute@developer.gserviceaccount.com\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].strip()\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set service account access for Vertex AI Pipelines\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\\n\\n! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up variables\n",
    "Next, set up some variables used throughout the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vertex AI constants\n",
    "Setup up the following constants for Vertex AI:\n",
    "\n",
    "- `API_ENDPOINT` : The Vertex AI API service endpoint for `Dataset`, `Model`, `Job`, `Pipeline` and `Endpoint` services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vertex AI Pipelines constants\n",
    "Setup up the following constants for Vertex AI Pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/rrs_{}\".format(BUCKET_NAME,TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, component)\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pipeline component to check for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    # this component builds a BQ view, which will be the underlying source for model\n",
    "    packages_to_install=[\"google-cloud-bigquery\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/create_input_view.yaml\",\n",
    ")\n",
    "\n",
    "def create_input_view(view_name: str, \n",
    "                      data_set_id: str, \n",
    "                      project_id: str,\n",
    "                      view_sql_statement: str                   \n",
    "):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    client = bigquery.Client(project=project_id)\n",
    "    dataset = client.dataset(data_set_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "    view_sql_statement = view_sql_statement\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(client, table_ref):\n",
    "        print(\"view already exists\")\n",
    "        \n",
    "    else: \n",
    "        print(\"view does not exists\")\n",
    "        #load sql from base_sql.txt.  This can be modified if you want to modify your query\n",
    "        content = view_sql_statement\n",
    "        create_base_feature_set_query = view_sql_statement\n",
    "\n",
    "        shared_dataset_ref = client.dataset(data_set_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = client.create_table(base_feature_set_view)  # API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_source_table = f\"bq://fsi-select-demo.vertex_forecasting_datasets.ds-rrs_consolidated_power_demand_ercot_weather_train_v2_{TIMESTAMP}\"\n",
    "bq_destination_table = f\"fsi-select-demo.vertex_forecasting_datasets.ds-rrs_consolidated_power_demand_ercot_weather_train_v2_{TIMESTAMP}\"\n",
    "\n",
    "time_column = \"date\"\n",
    "time_series_identifier_column = \"state\"\n",
    "target_column = \"RRS\"\n",
    "\n",
    "COLUMN_SPECS = {\n",
    "    time_column: \"timestamp\",\n",
    "    target_column: \"numeric\",\n",
    "    \"hr\": \"numeric\",\n",
    "    \"ave_temp\" : \"numeric\",\n",
    "    \"min_temp\" : \"numeric\",\n",
    "    \"max_temp\" : \"numeric\",\n",
    "    \"demand_mgwhr\" : \"numeric\"\n",
    "}\n",
    "\n",
    "@kfp.dsl.pipeline(name=\"rrs-automl-pipeline-training-v2\", description=\"RRS forecasting v2 with custom components\")\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID, \n",
    "    region: str = REGION,\n",
    "    bq_source: str = bq_source_table,\n",
    "    bq_sql_query: str = TRN_SQL,\n",
    "    bq_destination_table: str = bq_destination_table\n",
    "    ):\n",
    "    \n",
    "    from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "    from google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp as bq_JobOp\n",
    "    \n",
    "    create_input_view_op = create_input_view(view_name = VIEW_NAME,\n",
    "                                             data_set_id = DATA_SET_ID,\n",
    "                                             project_id = PROJECT_ID,\n",
    "                                             view_sql_statement = TRN_SQL\n",
    "                                             )\n",
    "        \n",
    "    dataset_create_op = gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "        project=project, \n",
    "        display_name=\"ppln2_rrs_pricing_texas\" + \"_\" + TIMESTAMP, \n",
    "        bq_source=bq_source,\n",
    "    ).after(create_input_view_op)\n",
    "\n",
    "    training_op = gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        optimization_objective=\"minimize-rmse\",\n",
    "        column_specs=COLUMN_SPECS,        \n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=target_column,\n",
    "        time_column=time_column,\n",
    "        time_series_identifier_column=time_series_identifier_column,\n",
    "        available_at_forecast_columns=[\"date\", \"hr\", \"ave_temp\", \"min_temp\", \"max_temp\", \"demand_mgwhr\"],\n",
    "        unavailable_at_forecast_columns=[target_column],\n",
    "        forecast_horizon=96,\n",
    "        context_window=96,\n",
    "        data_granularity_unit=\"hour\",\n",
    "        data_granularity_count=1,\n",
    "        budget_milli_node_hours=1000,\n",
    "        model_display_name=MODEL_DISPLAY_NAME,\n",
    "        export_evaluated_data_items=True,\n",
    "        export_evaluated_data_items_bigquery_destination_uri=\"bq://fsi-select-demo:automl_forecasts.pplin_predictions_evals\",\n",
    "        export_evaluated_data_items_override_destination=True,\n",
    "        validation_options=\"ignore-validation\",\n",
    "        training_fraction_split=0.8,\n",
    "        test_fraction_split=0.1, \n",
    "        validation_fraction_split= 0.1,\n",
    "    ).after(dataset_create_op)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler \n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"pipeline_outputs/ntbk v2 rrs_pricing_tx.json\".replace(\" \", \"_\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:tabular,forecast"
   },
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "create_automl_pipeline:tabular,forecast"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/rrs-automl-pipeline-training-v2-20220401212908?project=355426521391\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/355426521391/locations/us-central1/pipelineJobs/rrs-automl-pipeline-training-v2-20220401212908\n",
      "rm: cannot remove 'rrs_pricing_texas_pipeline.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    template_path=\"pipeline_outputs/ntbk v2 rrs_pricing_tx.json\".replace(\" \", \"_\"),\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run()\n",
    "\n",
    "! rm rrs_pricing_texas_pipeline.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the generated link to see your run in the Cloud Console.\n",
    "\n",
    "In the UI, many of the pipeline DAG nodes will expand or collapse when you click on them. Here is a partially-expanded view of the DAG (click image to see larger version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pipeline_v2_img.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_the_model:mbsdk",
    "tags": []
   },
   "source": [
    "## Review model evaluation scores\n",
    "After your model has finished training, you can review the evaluation scores for it.\n",
    "\n",
    "First, you need to get a reference to the new model. As with datasets, you can either use the reference to the model variable you created when you deployed the model or you can list all of the models in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/355426521391/locations/us-central1/models/7895254349478100992/evaluations/2528729093233765197\"\n",
      "metrics_schema_uri: \"gs://google-cloud-aiplatform/schema/modelevaluation/forecasting_metrics_1.0.0.yaml\"\n",
      "metrics {\n",
      "  struct_value {\n",
      "    fields {\n",
      "      key: \"meanAbsoluteError\"\n",
      "      value {\n",
      "        number_value: 590.431\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"meanAbsolutePercentageError\"\n",
      "      value {\n",
      "        number_value: 29.373018\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"rSquared\"\n",
      "      value {\n",
      "        number_value: 0.26673022\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"rootMeanSquaredError\"\n",
      "      value {\n",
      "        number_value: 3060.856\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"rootMeanSquaredLogError\"\n",
      "      value {\n",
      "        number_value: 1.0871344\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "create_time {\n",
      "  seconds: 1648853545\n",
      "  nanos: 416120000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get model resource ID\n",
    "models = aip.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
    "model = models[0]\n",
    "\n",
    "# Get a reference to the Model Service client\n",
    "client_options = aip.initializer.global_config.get_client_options()\n",
    "model_service_client = aip.gapic.ModelServiceClient(\n",
    "    client_options=client_options\n",
    ")\n",
    "\n",
    "model_evaluations = model_service_client.list_model_evaluations(\n",
    "    parent=model.resource_name\n",
    ")\n",
    "model_evaluation = list(model_evaluations)[0]\n",
    "print(model_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "## Send a batch prediction request\n",
    "\n",
    "Send a batch prediction to your deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "source": [
    "### Make the batch prediction request\n",
    "\n",
    "Now that your Model resource is trained, you can make a batch prediction by invoking the batch_predict() method, with the following parameters:\n",
    "\n",
    "- `job_display_name`: The human readable name for the batch prediction job.\n",
    "- `gcs_source`: A list of one or more batch request input files.\n",
    "- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.\n",
    "- `instances_format`: The format for the input instances, either 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
    "- `predictions_format`: The format for the output predictions, either 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
    "- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsi-select-demo\n"
     ]
    }
   ],
   "source": [
    "print(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "2c9d935c6ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dataset(DatasetReference('fsi-select-demo', 'automl_forecasts')) already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "bq_dataset_exists=False\n",
    "\n",
    "# bq_dataset_name_prefix = \"vertex_forecasting_predictions\"\n",
    "\n",
    "# batch_predict_bq_output_dataset_name = f\"ntbk_texas_rrs_pricing_predictions_{TIMESTAMP}\"\n",
    "# batch_predict_bq_output_dataset_name = f\"automl_forecasts.texas_rrs_pricing_predictions_{TIMESTAMP}\"\n",
    "batch_predict_bq_output_dataset_name = \"automl_forecasts\"\n",
    "batch_predict_bq_output_dataset_path = \"{}.{}\".format(\n",
    "    PROJECT_ID, batch_predict_bq_output_dataset_name\n",
    ")\n",
    "\n",
    "batch_predict_bq_output_uri_prefix = \"bq://{}.{}\".format(\n",
    "    PROJECT_ID, batch_predict_bq_output_dataset_name\n",
    ")\n",
    "\n",
    "# Must be the same region as batch_predict_bq_input_uri\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "bq_dataset = bigquery.Dataset(batch_predict_bq_output_dataset_path)\n",
    "dataset_region = \"us-central1\"  # @param {type : \"string\"}\n",
    "bq_dataset.location = dataset_region\n",
    "\n",
    "# Determine if the dataset already exists \n",
    "try:\n",
    "    client.get_dataset(bq_dataset)  # Make an API request.\n",
    "    print(\"Dataset {} already exists\".format(bq_dataset))\n",
    "    bq_dataset_exists = True\n",
    "except:\n",
    "    print(\"Dataset {} is not found\".format(bq_dataset))\n",
    "\n",
    "# Create the dataset if it does not exist\n",
    "if not bq_dataset_exists:\n",
    "    bq_dataset = client.create_dataset(bq_dataset)\n",
    "    print(\n",
    "        \"Created bigquery dataset {} in {}\".format(\n",
    "            batch_predict_bq_output_dataset_path, dataset_region\n",
    "        )\n",
    "    )\n",
    "    \n",
    "# If the dataset exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_bq_output_uri_prefix = \"bq://{}.{}\".format(\n",
    "    PROJECT_ID, batch_predict_bq_output_dataset_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n",
      "<google.cloud.aiplatform.jobs.BatchPredictionJob object at 0x7fc96892aa10> is waiting for upstream dependencies to complete.\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648\n",
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648')\n",
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/7509950840489115648?project=355426521391\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_DATASET_BQ_PATH = (\n",
    "    f\"bq://fsi-select-demo:vertex_forecasting_datasets.ds-rrs_consolidated_power_demand_ercot_weather_validation\"\n",
    ")\n",
    "\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=f\"ntbk_texas_rrs_pricing_predictions_{TIMESTAMP}\",\n",
    "    bigquery_source=PREDICTION_DATASET_BQ_PATH,\n",
    "    instances_format=\"bigquery\",\n",
    "    bigquery_destination_prefix=batch_predict_bq_output_uri_prefix,\n",
    "    predictions_format=\"bigquery\",\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "print(batch_prediction_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "### Wait for completion of batch prediction job\n",
    "\n",
    "Next, wait for the batch job to complete. Alternatively, you can set the parameter `sync` to `True` in the `batch_predict()` method to block until the batch prediction job is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob run completed. Resource name: projects/355426521391/locations/us-central1/batchPredictionJobs/7509950840489115648\n"
     ]
    }
   ],
   "source": [
    "batch_prediction_job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bq://fsi-select-demo.automl_forecasts\n"
     ]
    }
   ],
   "source": [
    "name_outputBQ_dataset = batch_prediction_job.output_info.bigquery_output_dataset\n",
    "print(name_outputBQ_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,forecast"
   },
   "source": [
    "### Get the predictions\n",
    "\n",
    "Next, get the results from the completed batch prediction job.\n",
    "\n",
    "The results are written to the designated BigQuery dataset. You may remove the run the cell below to view the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport tensorflow as tf\\n\\nbp_iter_outputs = batch_prediction_job.iter_outputs()\\n\\nfor output_b in bp_iter_outputs:\\n    print(output_b)\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "bp_iter_outputs = batch_prediction_job.iter_outputs()\n",
    "\n",
    "for output_b in bp_iter_outputs:\n",
    "    print(output_b)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,forecast"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport tensorflow as tf\\n\\nbp_iter_outputs = batch_prediction_job.iter_outputs()\\n\\nprediction_results = list()\\nfor blob in bp_iter_outputs:\\n    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\\n        prediction_results.append(blob.name)\\n\\ntags = list()\\nfor prediction_result in prediction_results:\\n    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\\n    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\\n        for line in gfile.readlines():\\n            print(line)\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # Commented out as the predictions were sent to BQ # '''\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "bp_iter_outputs = batch_prediction_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            print(line)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78080aa9088e"
   },
   "source": [
    "### Visualize the forecasts\n",
    "\n",
    "Lastly, follow the given link to visualize the generated forecasts in [Data Studio](https://support.google.com/datastudio/answer/6283323?hl=en).\n",
    "The code block included in this section dynamically generates a Data Studio link that specifies the template, the location of the forecasts, and the query to generate the chart. The data is populated from the forecasts generated earlier.\n",
    "\n",
    "#### Note\n",
    "For the purposes for the demo, a set of forecasts for 48 hours were run, and the results can be viewed [here](https://datastudio.google.com/c/u/0/reporting?params=%7B%22templateId%22:%22067f70d2-8cd6-4a4c-a099-292acd1053e8%22,%22ds0.connector%22:%22BIG_QUERY%22,%22ds0.projectId%22:%22355426521391%22,%22ds0.billingProjectId%22:%22355426521391%22,%22ds0.type%22:%22CUSTOM_QUERY%22,%22ds0.sql%22:%22SELECT%20%5Cn%20CAST(input.date%20as%20DATETIME)%20timestamp_col,%5Cn%20CAST(input.state%20as%20STRING)%20time_series_identifier_col,%5Cn%20CAST(input.RRS%20as%20NUMERIC)%20historical_values,%5Cn%20CAST(predicted_RRS.value%20as%20NUMERIC)%20predicted_values,%5Cn%20*%20%5CnFROM%20%60fsi-select-demo.vertex_forecasting_datasets.ds-rrs_consolidated_power_demand_ercot_weather_validation%60%20input%5CnLEFT%20JOIN%20%60fsi-select-demo.automl_forecasts.predictions_2022_03_10T05_08_00_228Z%60%20output%5CnON%5CnCAST(input.date%20as%20DATETIME)%20%3D%20CAST(output.date%20as%20DATETIME)%5CnAND%20CAST(input.state%20as%20STRING)%20%3D%20CAST(output.state%20as%20STRING)%22%7D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "id": "f82f00be2160",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://datastudio.google.com/c/u/0/reporting?params=%7B%22templateId%22%3A%22067f70d2-8cd6-4a4c-a099-292acd1053e8%22%2C%22ds0.connector%22%3A%22BIG_QUERY%22%2C%22ds0.projectId%22%3A%22fsi-select-demo%22%2C%22ds0.billingProjectId%22%3A%22fsi-select-demo%22%2C%22ds0.type%22%3A%22CUSTOM_QUERY%22%2C%22ds0.sql%22%3A%22SELECT+%5Cn+CAST%28input.date+as+DATETIME%29+timestamp_col%2C%5Cn+CAST%28input.state+as+STRING%29+time_series_identifier_col%2C%5Cn+CAST%28input.RRS+as+NUMERIC%29+historical_values%2C%5Cn+CAST%28predicted_RRS.value+as+NUMERIC%29+predicted_values%2C%5Cn+%2A+%5CnFROM+%60fsi-select-demo.vertex_forecasting_datasets.ds-rrs_consolidated_power_demand_ercot_weather_validation%60+input%5CnLEFT+JOIN+%60fsi-select-demo.automl_forecasts.predictions_2022_03_31T12_05_54_731Z%60+output%5CnON%5CnCAST%28input.date+as+DATETIME%29+%3D+CAST%28output.date+as+DATETIME%29%5CnAND+CAST%28input.state+as+STRING%29+%3D+CAST%28output.state+as+STRING%29%22%7D\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "tables = client.list_tables(batch_predict_bq_output_dataset_path)\n",
    "\n",
    "prediction_table_id = \"\"\n",
    "for table in tables:\n",
    "    if (\n",
    "        table.table_id.startswith(\"predictions_\")\n",
    "        and table.table_id > prediction_table_id\n",
    "    ):\n",
    "        prediction_table_id = table.table_id\n",
    "batch_predict_bq_output_uri = \"{}.{}\".format(\n",
    "    batch_predict_bq_output_dataset_path, prediction_table_id\n",
    ")\n",
    "\n",
    "\n",
    "def _sanitize_bq_uri(bq_uri):\n",
    "    if bq_uri.startswith(\"bq://\"):\n",
    "        bq_uri = bq_uri[5:]\n",
    "    return bq_uri.replace(\":\", \".\")\n",
    "\n",
    "\n",
    "def get_data_studio_link(\n",
    "    batch_prediction_bq_input_uri,\n",
    "    batch_prediction_bq_output_uri,\n",
    "    time_column,\n",
    "    time_series_identifier_column,\n",
    "    target_column,\n",
    "):\n",
    "    batch_prediction_bq_input_uri = _sanitize_bq_uri(batch_prediction_bq_input_uri)\n",
    "    batch_prediction_bq_output_uri = _sanitize_bq_uri(batch_prediction_bq_output_uri)\n",
    "    base_url = \"https://datastudio.google.com/c/u/0/reporting\"\n",
    "    query = (\n",
    "        \"SELECT \\\\n\"\n",
    "        \" CAST(input.{} as DATETIME) timestamp_col,\\\\n\"\n",
    "        \" CAST(input.{} as STRING) time_series_identifier_col,\\\\n\"\n",
    "        \" CAST(input.{} as NUMERIC) historical_values,\\\\n\"\n",
    "        \" CAST(predicted_{}.value as NUMERIC) predicted_values,\\\\n\"\n",
    "        \" * \\\\n\"\n",
    "        \"FROM `{}` input\\\\n\"\n",
    "        \"LEFT JOIN `{}` output\\\\n\"\n",
    "        \"ON\\\\n\"\n",
    "        \"CAST(input.{} as DATETIME) = CAST(output.{} as DATETIME)\\\\n\"\n",
    "        \"AND CAST(input.{} as STRING) = CAST(output.{} as STRING)\"\n",
    "    )\n",
    "    query = query.format(\n",
    "        time_column,\n",
    "        time_series_identifier_column,\n",
    "        target_column,\n",
    "        target_column,\n",
    "        batch_prediction_bq_input_uri,\n",
    "        batch_prediction_bq_output_uri,\n",
    "        time_column,\n",
    "        time_column,\n",
    "        time_series_identifier_column,\n",
    "        time_series_identifier_column,\n",
    "    )\n",
    "    params = {\n",
    "        \"templateId\": \"067f70d2-8cd6-4a4c-a099-292acd1053e8\",\n",
    "        \"ds0.connector\": \"BIG_QUERY\",\n",
    "        \"ds0.projectId\": PROJECT_ID,\n",
    "        \"ds0.billingProjectId\": PROJECT_ID,\n",
    "        \"ds0.type\": \"CUSTOM_QUERY\",\n",
    "        \"ds0.sql\": query,\n",
    "    }\n",
    "    params_str_parts = []\n",
    "    for k, v in params.items():\n",
    "        params_str_parts.append('\"{}\":\"{}\"'.format(k, v))\n",
    "    params_str = \"\".join([\"{\", \",\".join(params_str_parts), \"}\"])\n",
    "    return \"{}?{}\".format(base_url, urllib.parse.urlencode({\"params\": params_str}))\n",
    "\n",
    "\n",
    "print(\n",
    "    get_data_studio_link(\n",
    "        PREDICTION_DATASET_BQ_PATH,\n",
    "        batch_predict_bq_output_uri,\n",
    "        time_column,\n",
    "        time_series_identifier_column,\n",
    "        target_column,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/forecast_chart_v2.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "delete_dataset = True\n",
    "delete_pipeline = True\n",
    "delete_model = True\n",
    "delete_endpoint = True\n",
    "delete_batchjob = True\n",
    "delete_customjob = True\n",
    "delete_hptjob = True\n",
    "delete_bucket = True\n",
    "\n",
    "try:\n",
    "    if delete_model and \"DISPLAY_NAME\" in globals():\n",
    "        models = aip.Model.list(\n",
    "            filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "        )\n",
    "        model = models[0]\n",
    "        aip.Model.delete(model)\n",
    "        print(\"Deleted model:\", model)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    if delete_endpoint and \"DISPLAY_NAME\" in globals():\n",
    "        endpoints = aip.Endpoint.list(\n",
    "            filter=f\"display_name={DISPLAY_NAME}_endpoint\", order_by=\"create_time\"\n",
    "        )\n",
    "        endpoint = endpoints[0]\n",
    "        endpoint.undeploy_all()\n",
    "        aip.Endpoint.delete(endpoint.resource_name)\n",
    "        print(\"Deleted endpoint:\", endpoint)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "if delete_dataset and \"DISPLAY_NAME\" in globals():\n",
    "    if \"tabular\" == \"tabular\":\n",
    "        try:\n",
    "            datasets = aip.TabularDataset.list(\n",
    "                filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "            )\n",
    "            dataset = datasets[0]\n",
    "            aip.TabularDataset.delete(dataset.resource_name)\n",
    "            print(\"Deleted dataset:\", dataset)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    if \"tabular\" == \"image\":\n",
    "        try:\n",
    "            datasets = aip.ImageDataset.list(\n",
    "                filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "            )\n",
    "            dataset = datasets[0]\n",
    "            aip.ImageDataset.delete(dataset.resource_name)\n",
    "            print(\"Deleted dataset:\", dataset)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    if \"tabular\" == \"text\":\n",
    "        try:\n",
    "            datasets = aip.TextDataset.list(\n",
    "                filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "            )\n",
    "            dataset = datasets[0]\n",
    "            aip.TextDataset.delete(dataset.resource_name)\n",
    "            print(\"Deleted dataset:\", dataset)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    if \"tabular\" == \"video\":\n",
    "        try:\n",
    "            datasets = aip.VideoDataset.list(\n",
    "                filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "            )\n",
    "            dataset = datasets[0]\n",
    "            aip.VideoDataset.delete(dataset.resource_name)\n",
    "            print(\"Deleted dataset:\", dataset)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "try:\n",
    "    if delete_pipeline and \"DISPLAY_NAME\" in globals():\n",
    "        pipelines = aip.PipelineJob.list(\n",
    "            filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "        )\n",
    "        pipeline = pipelines[0]\n",
    "        aip.PipelineJob.delete(pipeline.resource_name)\n",
    "        print(\"Deleted pipeline:\", pipeline)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "if delete_bucket and \"BUCKET_NAME\" in globals():\n",
    "    ! gsutil rm -r $BUCKET_NAME\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk_automl_tabular_forecasting_batch.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
